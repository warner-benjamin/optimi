{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"optim\u012b","text":"<p>Fast, Modern, and Low Precision PyTorch Optimizers</p> <p>optimi enables accurate low precision training via Kahan summation, integrates gradient release and optimizer accumulation for additional memory efficiency, supports fully decoupled weight decay, and features fast implementations of modern optimizers.</p>"},{"location":"#low-precision-training-with-kahan-summation","title":"Low Precision Training with Kahan Summation","text":"<p>optimi optimizers can match the performance of mixed precision when training in pure BFloat16 by using Kahan summation.</p> <p></p> <p>Training in pure BFloat16 with Kahan summation can reduce non-activation training memory usage up to 37 to 45 percent when using an Adam optimizer. BFloat16 training can increase single GPU training speed up to 10 percent at the same batch size.</p>"},{"location":"#fast-triton-implementations","title":"Fast Triton Implementations","text":"<p>optimi's fused Triton optimizers are faster than PyTorch's fused Cuda optimizers, and nearly as fast as compiled optimizers without any hassle.</p> <p></p> <p>optimi's Triton backend supports modern NVIDIA (Ampere or newer), AMD, and Intel GPUs, and is enabled by default for all optimizers.</p>"},{"location":"#fully-decoupled-weight-decay","title":"Fully Decoupled Weight Decay","text":"<p>In addition to supporting PyTorch-style decoupled weight decay, optimi optimizers also support fully decoupled weight decay.</p> <p>Fully decoupled weight decay decouples weight decay from the learning rate, more accurately following Decoupled Weight Decay Regularization. This can help simplify hyperparameter tuning as the optimal weight decay is no longer tied to the learning rate.</p>"},{"location":"#gradient-release-fused-backward-and-optimizer-step","title":"Gradient Release: Fused Backward and Optimizer Step","text":"<p>optimi optimizers can perform the optimization step layer-by-layer during the backward pass, immediately freeing gradient memory.</p> <p>Unlike the current PyTorch implementation, optimi\u2019s gradient release optimizers are a drop-in replacement for standard optimizers and seamlessly work with exisiting hyperparmeter schedulers.</p>"},{"location":"#optimizer-accumulation-gradient-release-and-accumulation","title":"Optimizer Accumulation: Gradient Release and Accumulation","text":"<p>optimi optimizers can approximate gradient accumulation with gradient release by accumulating gradients into the optimizer states.</p>"},{"location":"#optimizers","title":"Optimizers","text":"<p>optimi implements the following optimizers:</p> <ul> <li>Adam</li> <li>AdamW</li> <li>Adan</li> <li>Lion</li> <li>RAdam</li> <li>Ranger</li> <li>SGD</li> <li>StableAdamW</li> </ul>"},{"location":"#install","title":"Install","text":"<p>optimi is available to install from pypi.</p> <pre><code>pip install torch-optimi\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>To use an optimi optimizer with Kahan summation and fully decoupled weight decay:</p> <pre><code>import torch\nfrom torch import nn\nfrom optimi import AdamW\n\n# create or cast model in low precision (bfloat16)\nmodel = nn.Linear(20, 1, dtype=torch.bfloat16)\n\n# initialize any optimi optimizer with parameters &amp; fully decoupled weight decay\n# Kahan summation is automatically enabled since model &amp; inputs are bfloat16\nopt = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5, decouple_lr=True)\n\n# forward and backward, casting input to bfloat16 if needed\nloss = model(torch.randn(20, dtype=torch.bfloat16))\nloss.backward()\n\n# optimizer step\nopt.step()\nopt.zero_grad()\n</code></pre> <p>To use with PyTorch-style weight decay with float32 or mixed precision:</p> <pre><code># create model\nmodel = nn.Linear(20, 1)\n\n# initialize any optimi optimizer with parameters\nopt = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n</code></pre> <p>To use with gradient release:</p> <pre><code># initialize any optimi optimizer with `gradient_release=True`\n# and call `prepare_for_gradient_release` on model and optimizer\nopt = AdamW(model.parameters(), lr=1e-3, gradient_release=True)\nprepare_for_gradient_release(model, opt)\n\n# setup a learning rate scheduler like normal\nscheduler = CosineAnnealingLR(opt, ...)\n\n# calling backward on the model will peform the optimzier step\nloss = model(torch.randn(20, dtype=torch.bfloat16))\nloss.backward()\n\n# optimizer step and zero_grad are no longer needed, and will\n# harmlessly no-op if called by an existing training framework\n# opt.step()\n# opt.zero_grad()\n\n# step the learning rate scheduler like normal\nscheduler.step()\n\n# optionally remove gradient release hooks when done training\nremove_gradient_release(model)\n</code></pre> <p>To use with optimizer accumulation:</p> <pre><code># initialize any optimi optimizer with `gradient_release=True`\n# and call `prepare_for_gradient_release` on model and optimizer\nopt = AdamW(model.parameters(), lr=1e-3, gradient_release=True)\nprepare_for_gradient_release(model, opt)\n\n# update model parameters every four steps after accumulating\n# gradients directly into the optimizer states\naccumulation_steps = 4\n\n# setup a learning rate scheduler for gradient accumulation\nscheduler = CosineAnnealingLR(opt, ...)\n\n# use existing PyTorch dataloader\nfor idx, batch in enumerate(dataloader):\n    # `optimizer_accumulation=True` accumulates gradients into\n    # optimizer states. set `optimizer_accumulation=False` to\n    # update parameters by performing a full gradient release step\n    opt.optimizer_accumulation = (idx+1) % accumulation_steps != 0\n\n    # calling backward on the model will peform the optimizer step\n    # either accumulating gradients or updating model parameters\n    loss = model(batch)\n    loss.backward()\n\n    # optimizer step and zero_grad are no longer needed, and will\n    # harmlessly no-op if called by an existing training framework\n    # opt.step()\n    # opt.zero_grad()\n\n    # step the learning rate scheduler after accumulating gradients\n    if not opt.optimizer_accumulation:\n        scheduler.step()\n\n# optionally remove gradient release hooks when done training\nremove_gradient_release(model)\n</code></pre>"},{"location":"#differences-from-pytorch","title":"Differences from PyTorch","text":"<p>optimi optimizers do not support compilation, differentiation, complex numbers, or have capturable versions.</p> <p>optimi Adam optimizers do not support AMSGrad and SGD does not support Nesterov momentum. Optimizers which debias updates (Adam optimizers and Adan) calculate the debias term per parameter group, not per parameter.</p>"},{"location":"foreach/","title":"ForEach Optimizer Implementations","text":"<p>Like PyTorch, optimi supports foreach implementations of all optimizers. Foreach optimizers can be significantly faster than the for-loop versions.</p> <p>Deprecation Notice: Foreach optimizers will be removed in a future release</p> <p>Foreach optimizers are deprecated and will be removed in a future release. They will not receive any new features.</p> <p>Use optimi\u2019s triton optimizers instead, as they are significantly faster than foreach optimizers.</p> <p>Foreach implementations can increase optimizer peak memory usage. optimi attempts to reduce this extra overhead by reusing the gradient buffer for temporary variables. If the gradients are required between the optimization step and gradient reset step, set <code>foreach=False</code> to use the for-loop implementation.</p> Note: Foreach Requires PyTorch 2.1+ <p>optimi\u2019s foreach implementations require PyTorch 2.1 or newer.</p> <p>If unspecified <code>foreach=None</code>, optimi will use the foreach implementation if training on a Cuda device.</p>"},{"location":"foreach/#example","title":"Example","text":"<p>To use a foreach implementation set <code>foreach=True</code> when initializing the optimizer.</p> <pre><code>import torch\nfrom torch import nn\nfrom optimi import AdamW\n\n# create model\nmodel = nn.Linear(20, 1, device=\"cuda\")\n\n# initialize any optmi optimizer with `foreach=True`\nopt = AdamW(model.parameters(), lr=1e-3, foreach=True)\n\n# forward and backward\nloss = model(torch.randn(20))\nloss.backward()\n\n# optimizer step is the foreach implementation\nopt.step()\nopt.zero_grad()\n</code></pre>"},{"location":"fully_decoupled_weight_decay/","title":"Fully Decoupled Weight Decay","text":"<p>In addition to supporting PyTorch-style decoupled weight decay, optimi optimizers also support fully decoupled weight decay.</p> <p>While PyTorch-style decoupled weight decay (hereafter referred to as \u201cdecoupled weight decay\u201d) decouples weight decay from the gradients, it explicitly couples weight decay with the learning rate \\(\\gamma_t\\lambda\\bm{\\theta}_{t-1}\\). This ties the optimal value of weight decay to the learning rate.</p> <p>In contrast, optimi\u2019s fully decoupled weight decay also decouples weight decay from the learning rate, more accurately following Decoupled Weight Decay Regularization by Loshchilov and Hutter.</p> <p>Fully decoupled weight decay is scaled by the relative learning rate \\((\\gamma_t/\\gamma_\\text{max})\\lambda\\bm{\\theta}_{t-1}\\) so applied weight decay will still follow the learning rate schedule.</p> Note: Implementation Inspired by Composer <p>optimi\u2019s fully decoupled weight decay implementation was inspired by Mosaic Composer\u2019s Decoupled Weight Decay.</p> <p>By default, optimi optimizers do not use fully decoupled weight decay for compatibility with existing PyTorch code.</p> <p>Enable fully decoupled weight decay by setting <code>decouple_lr=True</code> when initialing an optimi optimizer. If the initial learning rate <code>lr</code> isn\u2019t the maximum scheduled learning rate, pass it to <code>max_lr</code>.</p>"},{"location":"fully_decoupled_weight_decay/#hyperparameters","title":"Hyperparameters","text":"<p>Since fully decoupled weight decay is not multiplied by the learning rate each step, the optimal value for fully decoupled weight decay is smaller than decoupled weight decay.</p> <p>For example, to match AdamW\u2019s default decoupled weight decay of 0.01 with a maximum learning rate of \\(1\\times10^{-3}\\), set weight decay to \\(1\\times10^{-5}\\) when using fully decoupled weight decay.</p> <p>By default, optimi optimizers assume <code>lr</code> is the maximum scheduled learning rate. This allows the applied weight decay \\((\\gamma_t/\\gamma_\\text{max})\\lambda\\bm{\\theta}_{t-1}\\) to match the learning rate schedule. Set <code>max_lr</code> if this is not the case.</p>"},{"location":"fully_decoupled_weight_decay/#example","title":"Example","text":"<pre><code>import torch\nfrom torch import nn\nfrom optimi import AdamW\n\n# create model\nmodel = nn.Linear(20, 1, dtype=torch.bfloat16)\n\n# initialize any optimi optimizer useing `decouple_lr=True` to enable fully\n# decoupled weight decay. note `weight_decay` is lower then the default of 1e-2\nopt = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5, decouple_lr=True)\n\n# model is optimized using fully decoupled weight decay\nloss = model(torch.randn(20, dtype=torch.bfloat16))\nloss.backward()\n\nopt.step()\nopt.zero_grad()\n</code></pre>"},{"location":"fully_decoupled_weight_decay/#algorithm","title":"Algorithm","text":"<p>The algorithm below shows the difference between PyTorch\u2019s AdamW and optimi\u2019s Adam with fully decoupled weight decay.</p> \\[ \\begin{aligned}     &amp;\\rule{105mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textcolor{#009ddb}{\\text{PyTorch\u2019s AdamW}} \\: \\text{\\&amp;} \\: \\textcolor{#9a3fe4}{\\text{Adam with fully decoupled weight decay}}\\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\text{ (epsilon)};\\\\     &amp;\\hspace{17.25mm} \\gamma_\\text{max} \\: \\text{(maximum learning rate)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}\\\\[-0.5em]     &amp;\\rule{105mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1})\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t \\bigl( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) \\textcolor{#009ddb}{+ \\lambda\\bm{\\theta}_{t-1}} \\bigr)\\textcolor{#9a3fe4}{- (\\gamma_t/\\gamma_\\text{max})\\lambda\\bm{\\theta}_{t-1}}\\\\[-0.5em]     &amp;\\rule{105mm}{0.4pt}\\\\ \\end{aligned} \\] <p>This difference applies to all optimi optimizers which implement both decoupled and fully decoupled weight decay.</p>"},{"location":"gradient_release/","title":"Gradient Release","text":"<p>Fused Backward Pass and Optimizer Step</p> <p>Gradient release reduces training memory by limiting gradients to one layer at any given time. Unlike PyTorch\u2019s implementation, optimi\u2019s gradient release is fully compatible with both existing learning rate and optimizer schedulers and existing training frameworks.</p> <p>During the backward pass, each model layer calculates its gradients, performs the optimizer step, and clears the gradients before proceeding to the backward pass for the next layer. This fused backward and optimizer step can reduce non-activation memory usage by ~25 percent for an Adam optimizer.</p> <p>Gradient release can also be combined with other techniques such as Kahan summation or activation checkpointing for further memory savings.</p> Note: Gradient Release Requires PyTorch 2.1+ <p>Gradient release requires PyTorch 2.1 or newer.</p> <p>Gradient release was proposed by Pudipeddi et al in Training Large Neural Networks with Constant Memory using a New Execution Algorithm and was enabled by PyTorch\u2019s <code>register_post_accumulate_grad_hook</code>.</p>"},{"location":"gradient_release/#limitations-and-workarounds","title":"Limitations and Workarounds","text":"<p>Since gradient release immediately frees the gradient during the backward pass, features which rely on persistent gradients like AMP's <code>GradScaler</code>, gradient clipping, or gradient accumulation won\u2019t work.</p> <p>Important: Gradient Release is Incompatible with FP16 Mixed Precision</p> <p>Gradient release is incompatible with Float16 Automatic Mixed Precision since PyTorch's <code>GradScaler</code> requires access to the entire model's gradients for the optimizer step.</p> <p>Use BFloat16 Automatic Mixed Precision instead.</p> <p>The recommended workaround for gradient clipping is to use StableAdamW instead of Adam or AdamW, as StableAdamW removes the need for gradient clipping by porting Adafactor\u2019s update clipping into AdamW.</p> Tip: Use Optimizer Accumulation to Approximate Gradient Accumulation <p>optimi's optimizer accumulation approximates gradient accumlation by defering parameter updates while accumulating gradients directly into the optimizer states.</p> <p>One potential workaround for gradient accumulation is to increase the optimizer\u2019s momentum or \\(\\beta_1\\) to approximate accumulating gradients across multiple batches.</p>"},{"location":"gradient_release/#example","title":"Example","text":"<p>Using optimi\u2019s gradient release requires two steps: initializing the optimizer with <code>gradient_release=True</code> and calling <code>prepare_for_gradient_release</code> on both the model and optimizer.</p> <pre><code>import torch\nfrom torch import nn\nfrom optimi import AdamW\n\n# create or cast model in low precision (bfloat16)\nmodel = nn.Linear(20, 1, dtype=torch.bfloat16)\n\n# initialize any optimi optimizer with `gradient_release=True`\n# and call `prepare_for_gradient_release` on model and optimizer\nopt = AdamW(model.parameters(), lr=1e-3, gradient_release=True)\nprepare_for_gradient_release(model, opt)\n\n# setup a learning rate scheduler like normal\nscheduler = CosineAnnealingLR(opt, ...)\n\n# calling backward on the model will peform the optimzier step\nloss = model(torch.randn(20, dtype=torch.bfloat16))\nloss.backward()\n\n# optimizer step and zero_grad are no longer needed, and will\n# harmlessly no-op if called by an existing training framework\n# opt.step()\n# opt.zero_grad()\n\n# step the learning rate scheduler like normal\nscheduler.step()\n\n# optionally remove gradient release hooks when done training\nremove_gradient_release(model)\n</code></pre>"},{"location":"kahan_summation/","title":"Low Precision Training with Kahan Summation","text":"<p>While training models in low precision (Float16 or BFloat16) usually differs from training in full precision (Float32) or mixed precision, optimi optimizers can match the performance of mixed precision when training in pure BFloat16 by using Kahan summation<sup>1</sup>.</p> <p></p> <p>Training in low precision reduces non-activation memory usage up to ~46 percent and can increase training speed up to ~30 percent relative to mixed precision training.</p> <p>Using Kahan summation for accurate BFloat16 training is as simple as replacing a PyTorch optimizer with its optimi equivalent and casting the model to BFloat16 instead of using mixed precision.</p> <p>Tip: Keep a Few Layers in Float32</p> <p>When training in BFloat16, keep rotary buffers, rotary calculations, and token embedding layers in Float32, as these benefit from full precision. This results in a small memory increase and speed decrease, but can help guarantee equivalent results with mixed precision training.</p> <p>optimi's <code>to_low_precision</code> simplifies keeping these layers in Float32.</p> <p>By default, optimi optimizers will automatically use Kahan summation for any layers training in low precision. Set <code>kahan_sum=False</code> to disable.</p>"},{"location":"kahan_summation/#mixed-precision","title":"Mixed Precision","text":"<p>While implementations details can differ, mixed precision works by running a forward pass in low precision, automatically switching to full precision per layer as needed, and then accumulating gradients during the backward pass in Float32. The optimizer step runs in full precision.</p> <p>The hybrid precision setup of mixed precision enables the faster operations and lower memory usage of low precision while keeping the convergence of full precision.</p>"},{"location":"kahan_summation/#kahan-summation","title":"Kahan Summation","text":"<p>Kahan summation<sup>2</sup> is a technique to reduce the numerical error of adding multiple low precision numbers by accumulating errors in a separate compensation buffer. The addition of the compensation buffer increases the effective summation precision by the precision of the compensation buffer.</p> <p>Using Kahan summation to improve low precision model training was first introduced by Zamirai et al in Revisiting BFloat16 Training. Zamirai et al discovered the primary source of numerical error from low precision training is during the optimizer\u2019s model weight update step. They add Kahan summation to the SGD &amp; AdamW weight update steps to reduce the update\u2019s numerical inaccuracy, increasing low precision training to the equivalent of full precision training across tested models.</p> Note: Implementation Inspired by TorchDistX <p>optimi\u2019s Kahan summation implementation was directly inspired by TorchDistX\u2019s <code>AnyPrecisionAdamW</code> optimizer.</p> <p>For more details, see the algorithm and explanation sections.</p>"},{"location":"kahan_summation/#memory-savings","title":"Memory Savings","text":"<p>Training in pure BFloat16 with Kahan summation can reduce non-activation training memory usage up to 37 to 45 percent when using an Adam optimizer, as Table 1 shows below.</p> Table 1: Adam Per Parameter Memory Usage, Excluding Activations Buffer Mixed Precision BFloat16 + Kahan Sum BFloat16 BF16 Model Weights (if used) 2 bytes 2 bytes 2 bytes FP32 Model Weights 4 bytes - - Gradients 4 bytes 2 bytes 2 bytes Distributed Buffer (optional) 4 bytes 2 bytes 2 bytes Momentum 4 bytes 2 bytes 2 bytes Variance 4 bytes 2 bytes 2 bytes Kahan Compensation - 2 bytes - Total 16-22 bytes 10-12 bytes 8-10 bytes <p>Calculating the total memory savings depends on activations and batch size, mixed precision implementation details, and the optimizer used, to name a few variables.</p> <p>optimi reduces potential extra memory overhead of Kahan summation by reusing the gradient buffer for temporary variables.</p>"},{"location":"kahan_summation/#training-speedup","title":"Training Speedup","text":"<p>Training in BFloat16 instead of mixed precision results in up to a ~10% speedup on a single GPU, up to a ~20% speedup with two GPUs, and up to a ~30% speedup with multiple GPUs<sup>3</sup>.</p>"},{"location":"kahan_summation/#example","title":"Example","text":"<p>Using Kahan summation with an optimi optimizer only requires a casting a model and optionally input into low precision (BFloat16 or Float16). Since Kahan summation is applied layer by layer, it works for models with standard and low precision weights.</p> <pre><code>import torch\nfrom torch import nn\nfrom optimi import AdamW\n\n# create or cast some model layers in low precision (bfloat16)\nmodel = nn.Linear(20, 1, dtype=torch.bfloat16)\n\n# initialize any optmi optimizer with low precsion parameters\n# Kahan summation is enabled since some model layers are bfloat16\nopt = AdamW(model.parameters(), lr=1e-3)\n\n# forward and backward, casting input to bfloat16 if needed\nloss = model(torch.randn(20, dtype=torch.bfloat16))\nloss.backward()\n\n# optimizer step automatically uses Kahan summation for low precision layers\nopt.step()\nopt.zero_grad()\n</code></pre> <p>To disable Kahan Summation pass <code>kahan_summation=False</code> on optimizer initialization.</p>"},{"location":"kahan_summation/#algorithm","title":"Algorithm","text":"<p>SGD with Kahan summation.</p> \\[ \\begin{aligned}     &amp;\\rule{90mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textcolor{#009ddb}{\\textbf{SGD}} \\: \\textcolor{#9a3fe4}{\\text{with Kahan summation}}\\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)};\\\\     &amp;\\hspace{17.25mm} \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\: \\lambda \\: \\text{(weight decay)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\textcolor{#9a3fe4}{\\bm{k}_{0} \\leftarrow \\bm{0}}\\\\[-0.5em]     &amp;\\rule{90mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1}) - \\lambda\\bm{\\theta}_{t-1}\\\\[0.5em]         &amp;\\hspace{10mm} \\textcolor{#009ddb}{\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t\\bm{g}_t}\\\\[0.3em]         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\bm{u}_t \\leftarrow \\bm{k}_{t-1} - \\gamma_t\\bm{g}_t}\\\\         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} + \\bm{u}_t}\\\\         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\bm{k}_t \\leftarrow \\bm{u}_t + (\\bm{\\theta}_{t-1} - \\bm{\\theta}_t)}\\\\[-0.5em]     &amp;\\rule{90mm}{0.4pt}\\\\ \\end{aligned} \\] <p>This shows the optimi implementation of Kahan summation optimizers, which is equivalent to the Revisiting BFloat16 Training formulation.</p>"},{"location":"kahan_summation/#explanation","title":"Explanation","text":"<p>optimi optimizers with Kahan summation modify the base optimizers by introducing a compensation buffer \\(k\\) to mitigate numerical errors from training in low precision.</p> <p>Using SGD as an example, the SGD parameter update is straightforward: \\(\\textcolor{#009ddb}{\\bm{\\theta}_t \u2190 \\bm{\\theta}_{t-1} - \\gamma_t\\bm{g}_t}\\). Where \\(\\theta\\) is the model parameters at steps \\(t-1\\) and \\(t\\), and \\(\\gamma_t\\) and \\(g_t\\) are the learning rate and gradient at step \\(t\\), respectively.</p> <p>SGD with Kahan summation expands the single update model parameter step to three steps:</p> <ol> <li>\\(\\textcolor{#9a3fe4}{\\bm{u}_t \u2190 \\bm{k}_{t-1} - \\gamma_t\\bm{g}_t}\\): First, an intermediate update \\(u_t\\) is computed from the prior compensation buffer \\(k_{t-1}\\). This allows the current parameter update to account for any precision errors in last parameter update.</li> <li>\\(\\textcolor{#9a3fe4}{\\bm{\\theta}_t \u2190 \\bm{\\theta}_{t-1} + \\bm{u}_t}\\): The parameter update uses the error compensated update \\(u_t\\) instead of directly subtracting \\(\\gamma_t g_t\\).</li> <li>\\(\\textcolor{#9a3fe4}{\\bm{k}_t \u2190 \\bm{u}_t + (\\bm{\\theta}_{t-1} - \\bm{\\theta}_t)}\\): Finally, the compensation buffer is updated to stores the difference between the intended update (\\(u_t\\)) and the actual change in the parameters \\((\\theta_{t-1} - \\theta_t)\\), capturing any errors from operating in low precision numerical types.</li> </ol> <p>These Kahan summation steps allow optimi optimizers to nearly reach or match the performance of mixed precision when training in low precision.</p> <ol> <li> <p>Plot shows a simplified modded-nanogpt 160M model, trained on 1B FineWeb-Edu tokens, with RoPE and token embedding layers in Float32 and all other layers in BFloat16.\u00a0\u21a9</p> </li> <li> <p>Also known as Kahan\u2013Babu\u0161ka summation or compensated summation.\u00a0\u21a9</p> </li> <li> <p>Pure BFloat16 training can increase distributed training speed more then single GPU due to the halved bandwidth cost. Observed results may differ based on GPU connectivity and effectiveness of computation-communication overlap. Maximum observed speed increase was on consumer GPUs.\u00a0\u21a9</p> </li> </ol>"},{"location":"optimizer_accumulation/","title":"Optimizer Accumulation","text":"<p>Gradient Release with Approximate Gradient Accumulation</p> <p>Gradient accumulation reduces training memory by splitting a batch into micro-batches and accumulating micro-batch gradients into the larger batch. Gradient release reduces training memory by limiting gradients to one layer at any given time. Optimizer accumulation unifies these two disparate approaches by accumulating gradients directly into optimizer states while performing gradient release.</p> <p>During the backward pass, each model layer calculates its gradients, performs a partial optimizer step, and clears the gradients before proceeding to the backward pass for the next layer. The partial optimizer step accumulates gradients by updating the optimizer state but not modifying the model weights. After multiple gradients have been accumulated into optimizer states, a normal optimizer step is ran updating the model parameters with the accumulated states.</p> <p>Optimizer accumulation can reduce non-activation memory usage by ~40 percent compared to an Adam optimizer with gradient accumulation. Optimizer accumulation can also be combined with other techniques such as Kahan summation or activation checkpointing for further memory savings.</p> Note: Optimizer Accumulation Requires PyTorch 2.1+ <p>Optimizer accumulation requires PyTorch 2.1 or newer.</p> <p>Optimizer accumulation was proposed by Zhang et al in AdamAccumulation to Reduce Memory Footprints of both Activations and Gradients for Large-scale DNN Training. optimi\u2019s implementation enables AdamAccumulation for all optimi optimizers<sup>1</sup>.</p> <p>Zhang et al report that models trained with an AdamAccumulation over eight micro-batches match models trained via Adam with gradient accumulation over eight micro-batches.</p>"},{"location":"optimizer_accumulation/#limitations-and-workarounds","title":"Limitations and Workarounds","text":"<p>Since optimizer accumulation immediately frees the gradient during the backward pass, features which rely on persistent gradients like AMP's <code>GradScaler</code>, gradient clipping, or gradient accumulation won\u2019t work. L2 weight decay also shouldn\u2019t be used with optimizer accumulation.</p> <p>Important: Optimizer Accumulation is Incompatible with FP16 Mixed Precision</p> <p>Optimizer accumulation is incompatible with Float16 Automatic Mixed Precision since PyTorch's <code>GradScaler</code> requires access to the entire model's gradients for the optimizer step.</p> <p>Use BFloat16 Automatic Mixed Precision instead.</p> <p>The recommended workaround for gradient clipping is to use StableAdamW instead of Adam or AdamW, as StableAdamW removes the need for gradient clipping by porting Adafactor\u2019s update clipping into AdamW.</p> <p>Important: Don't use L2 Weight Decay with Optimizer Accumulation</p> <p>optimi applies weight decay on the full optimization step. Since L2 weight decay operates on the gradients, it would only be applied on the last gradient instead of all gradients.</p> <p>Use decoupled or fully decoupled weight decay instead.</p> <p>Because the gradients are accumulated into the optimizer states, applying beta and momentum terms, optimizer accumulation approximates gradient accumulation.</p>"},{"location":"optimizer_accumulation/#example","title":"Example","text":"<p>Using optimi\u2019s optimizer accumulation requires three steps: initializing the optimizer with <code>gradient_release=True</code>, calling <code>prepare_for_gradient_release</code> on both the model and optimizer, and setting <code>optimizer.optimizer_accumulation</code> to True or False to accumulation gradients or perform a full optimizer step, respectively.</p> <p>Like gradient accumulation, set <code>optimizer_accumulation=True</code> before the backward step while accumulating gradients and <code>optimizer_accumulation=False</code> when model parameters are to be updated by the full optimizer step.</p> <pre><code>import torch\nfrom torch import nn\nfrom optimi import AdamW\n\n# create or cast model in low precision (bfloat16)\nmodel = nn.Linear(20, 1, dtype=torch.bfloat16)\n\n# initialize any optimi optimizer with `gradient_release=True`\n# and call `prepare_for_gradient_release` on model and optimizer\nopt = AdamW(model.parameters(), lr=1e-3, gradient_release=True)\nprepare_for_gradient_release(model, opt)\n\n# update model parameters every four steps after accumulating\n# gradients directly into the optimizer states\naccumulation_steps = 4\n\n# setup a learning rate scheduler for gradient accumulation\nscheduler = CosineAnnealingLR(opt, ...)\n\n# use existing PyTorch dataloader\nfor idx, batch in enumerate(dataloader):\n    # `optimizer_accumulation=True` accumulates gradients into\n    # optimizer states. set `optimizer_accumulation=False` to\n    # update parameters by performing a full gradient release step\n    opt.optimizer_accumulation = (idx+1) % accumulation_steps != 0\n\n    # calling backward on the model will peform the optimizer step\n    # either accumulating gradients or updating model parameters\n    loss = model(batch)\n    loss.backward()\n\n    # optimizer step and zero_grad are no longer needed, and will\n    # harmlessly no-op if called by an existing training framework\n    # opt.step()\n    # opt.zero_grad()\n\n    # step the learning rate scheduler after accumulating gradients\n    if not opt.optimizer_accumulation:\n        scheduler.step()\n\n# optionally remove gradient release hooks when done training\nremove_gradient_release(model)\n</code></pre> <ol> <li> <p>While optimizer accumulation is noisy compared to gradient accumulation, SGD's optimizer accumulation results are significantly nosier then all other optimizers.\u00a0\u21a9</p> </li> </ol>"},{"location":"triton/","title":"Triton Optimizer Implementations","text":"<p>optimi's vertically fused Triton optimizers are faster than PyTorch's fused Cuda optimizers, and nearly as fast as PyTorch's compiled optimizers without any hassle<sup>1</sup>.</p> <p></p> <p>If unspecified <code>triton=None</code> and <code>foreach=None</code>, optimi will use the Triton implementation by default if training on a modern NVIDIA, AMD, or Intel GPU<sup>2</sup>.</p> Note: Triton Optimizers Requires PyTorch 2.6+ <p>optimi\u2019s Triton implementations require PyTorch 2.6 or newer. It's recommended to use the latest version of PyTorch and Triton.</p> <p>The Triton backend is compatible with gradient release and optimizer accumulation.</p>"},{"location":"triton/#example","title":"Example","text":"<p>optimi will use the Triton backend by default on a supported GPU. To disable this behavior set <code>triton=False</code> when initializing the optimizer.</p> <pre><code>import torch\nfrom torch import nn\nfrom optimi import AdamW\n\n# create model\nmodel = nn.Linear(20, 1, device=\"cuda\")\n\n# models on a supported GPU will default to `triton=True`\nopt = AdamW(model.parameters(), lr=1e-3)\n\n# or initialize any optimi optimizer with `triton=True`\nopt = AdamW(model.parameters(), lr=1e-3, triton=True)\n\n# forward and backward\nloss = model(torch.randn(20))\nloss.backward()\n\n# optimizer step is the Triton implementation\nopt.step()\nopt.zero_grad()\n</code></pre> <ol> <li> <p>Compiling optimizers requires change to the training loop which might not be supported by your training framework of choice, and any dynamic hyperparemters such as the learning rate need to be passed as Tensors or the optimizer will recompile every step.\u00a0\u21a9</p> </li> <li> <p>A GPU supporting bfloat16. Ampere or newer (A100 or RTX 3000 series), or any supported AMD or Intel GPU.\u00a0\u21a9</p> </li> </ol>"},{"location":"utils/","title":"Utilities","text":"<p><code>param_groups_weight_decay</code> is adapted from timm's optimizer factory methods.</p>"},{"location":"utils/#optimi.utils.param_groups_weight_decay","title":"param_groups_weight_decay","text":"<pre><code>param_groups_weight_decay(\n    model, weight_decay=0.01, additional_layers=None\n)\n</code></pre> <p>Creates parameter groups excluding bias and normalization layers from weight decay.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model to create parameter groups for</p> required <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient applied to eligible parameters (default: 1e-2)</p> <code>0.01</code> <code>additional_layers</code> <code>Iterable[str] | None</code> <p>Iterable of layer name substrings to exclude from weight decay. Any parameter whose name contains one of these substrings will be excluded from weight decay.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of two parameter group dictionaries, one with and one without weight decay.</p>"},{"location":"utils/#examples","title":"Examples","text":"<p><code>param_groups_weight_decay</code> takes a model and returns two optimizer parameter group dictionaries. One with bias and normalization terms without weight decay and another dictionary with the rest of the model parameters with weight decay. The <code>weight_decay</code> passed to <code>param_groups_weight_decay</code> will override the optimizer's default weight decay.</p> <pre><code>params = param_groups_weight_decay(model, weight_decay=1e-5)\noptimizer = StableAdamW(params, decouple_lr=True)\n</code></pre> <p><code>additional_layers</code> parameter allows you to specify additional layer names or name substrings that should be excluded from weight decay. This is useful for excluding specific layers like token embeddings which also benefit from not having weight decay applied.</p> <p>The parameter accepts an iterable of strings, where each string is matched as a substring against the full parameter name (as returned by <code>model.named_parameters()</code>).</p> <pre><code>class MiniLM(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.tok_embeddings = nn.Embedding(1000, 20)\n        self.pos_embeddings = nn.Embedding(100, 20)\n        self.norm = nn.LayerNorm(20)\n        self.layer1 = nn.Linear(20, 30)\n        self.layer2 = nn.Linear(30, 1000)\n\nmodel = MiniLM()\n\n# Exclude token embeddings from weight decay in addition to bias and normalization layers\nparams = param_groups_weight_decay(\n    model,\n    weight_decay=1e-5,\n    additional_layers=[\"tok_embeddings\"]\n)\n</code></pre>"},{"location":"utils/#optimi.utils.to_low_precision","title":"to_low_precision","text":"<pre><code>to_low_precision(\n    model,\n    dtype=torch.bfloat16,\n    device=None,\n    fp32_modules=nn.Embedding,\n    fp32_buffers=(\"rope\", \"rotary\"),\n)\n</code></pre> <p>Cast model to a low-precision dtype keeping select modules and buffers in float32.</p> <p>Keeps all parameters and buffers of <code>fp32_modules</code> in float32 (default: <code>nn.Embedding</code>). Keeps only buffers in float32 for modules whose qualified name contains any keyword in <code>fp32_buffers</code> or whose type is listed there (useful for RoPE-style buffers). Casts everything else to <code>dtype</code> and optionally moves to <code>device</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Module to cast in-place.</p> required <code>dtype</code> <code>dtype</code> <p>Target dtype (default: <code>torch.bfloat16</code>).</p> <code>bfloat16</code> <code>device</code> <code>device | str | None</code> <p>Optional target device (default: None).</p> <code>None</code> <code>fp32_modules</code> <code>type[Module] | Iterable[type[Module]] | None</code> <p>Modules to keep in float32 (default: <code>nn.Embedding</code>).</p> <code>Embedding</code> <code>fp32_buffers</code> <code>str | type[Module] | Iterable[str] | Iterable[type[Module]] | None</code> <p>Names and/or modules to keep buffers in float32 (default: (\"rope\", \"rotary\")).</p> <code>('rope', 'rotary')</code> <p>Returns:</p> Type Description <code>Module</code> <p>The input <code>model</code> (cast in-place).</p>"},{"location":"utils/#examples_1","title":"Examples","text":"<pre><code>from torch import nn\nfrom optimi.utils import to_low_precision\n\nmodel = nn.Sequential(\n    nn.Embedding(1000, 128),\n    nn.Linear(128, 128),\n)\n\nto_low_precision(model, dtype=torch.bfloat16)\n</code></pre> <p>Override which modules remain float32 (e.g., only keep <code>LayerNorm</code> in float32):</p> <pre><code>to_low_precision(model, dtype=torch.bfloat16, fp32_modules=(nn.LayerNorm,))\n</code></pre> <p>Disable RoPE/rotary buffer preservation so they are also cast to low precision:</p> <pre><code>to_low_precision(model, dtype=torch.bfloat16, fp32_buffers=None)\n</code></pre> <p>Cast and move to a device in one pass:</p> <pre><code>to_low_precision(model, dtype=torch.bfloat16, device=\"cuda\")\n</code></pre> <p>For details on using <code>prepare_for_gradient_release</code>, please see the gradient release docs.</p> <p>For details on using <code>remove_gradient_release</code>, please see the gradient release docs.</p>"},{"location":"utils/#optimi.gradientrelease.prepare_for_gradient_release","title":"prepare_for_gradient_release","text":"<pre><code>prepare_for_gradient_release(\n    model, optimizer, ignore_existing_hooks=False\n)\n</code></pre> <p>Register post_accumulate_grad_hooks on parameters for the gradient release optimization step.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to register post_accumulate_grad_hooks. Only registers on parameters with <code>requires_grad=True</code>.</p> required <code>optimizer</code> <code>OptimiOptimizer</code> <p>Optimizer providing the fused optimizer step during the backward pass. Requires optimizer to be initialized with <code>gradient_release=True</code></p> required <code>ignore_existing_hooks</code> <code>bool</code> <p>If True, ignores existing post_accumulate_grad_hooks on parameters and registers gradient release hooks (default: False)</p> <code>False</code>"},{"location":"utils/#optimi.gradientrelease.remove_gradient_release","title":"remove_gradient_release","text":"<pre><code>remove_gradient_release(model)\n</code></pre> <p>Removes post_accumulate_grad_hooks created by <code>prepare_for_gradient_release</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to remove gradient release post_accumulate_grad_hooks from.</p> required"},{"location":"which_optimizer/","title":"Which Optimizer Should I Use?","text":"<p>This guide is meant to provide a quick overview for choosing an optimi optimizer.</p> <p>All optimi optimizers support training in pure BFloat16 precision<sup>1</sup> using Kahan summation, which can help match Float32 optimizer performance with  mixed precision while reducing memory usage and increasing training speed.</p>"},{"location":"which_optimizer/#tried-and-true","title":"Tried and True","text":"<p>There\u2019s a reason AdamW is the default optimizer of deep learning. It performs well across multiple domains, model architectures, and batch sizes. Most optimizers claiming to outperform AdamW usually do not after careful analysis and experimentation.</p> <p>Consider reducing the \\(\\beta_2\\) term if training on large batch sizes or observing training loss spikes<sup>2</sup>.</p>"},{"location":"which_optimizer/#drop-in-replacement","title":"Drop-in Replacement","text":"<p>If using gradient clipping during training or experience training loss spikes, try replacing AdamW with StableAdamW. StableAdamW applies AdaFactor style update clipping to AdamW, stabilizing training loss and removing the need for gradient clipping.</p> <p>StableAdamW can outperform AdamW with gradient clipping on downstream tasks.</p>"},{"location":"which_optimizer/#low-memory-usage","title":"Low Memory Usage","text":"<p>If optimizer memory usage is important and optimi\u2019s Kahan summation doesn\u2019t alleviate optimizer memory usage or even more memory savings are desired, try optimi\u2019s two low memory optimizers: Lion and SGD.</p> <p>Lion uses one memory buffer for both momentum and the update step, reducing memory usage compared to AdamW. While reviews are mixed, Lion can match AdamW in some training scenarios.</p> <p>Prior to Adam and AdamW, SGD was the default optimizer for deep learning. SGD with Momentum can match or outperform AdamW on some tasks but can require more hyperparameter tuning. Consider using SGD with decoupled weight decay, it can lead to better results than L2 regularization.</p>"},{"location":"which_optimizer/#potential-upgrade","title":"Potential Upgrade","text":"<p>Adan can outperform AdamW at the expense of extra memory usage due to using two more buffers then AdamW. Consider trying Adan if optimizer memory usage isn\u2019t a priority, or when finetuning.</p>"},{"location":"which_optimizer/#small-batch-cnn","title":"Small Batch CNN","text":"<p>Ranger can outperform AdamW when training or finetuning on small batch sizes (~512 or less) with convolutional neural networks. It does use one more buffer then AdamW. Ranger performs best with a flat learning rate followed by a short learning rate decay.</p> <ol> <li> <p>Or BFloat16 with embedding and RoPE layers in Float32.\u00a0\u21a9</p> </li> <li> <p>This setting is mentioned in Sigmoid Loss for Language Image Pre-Training, although it is common knowledge in parts of the deep learning community.\u00a0\u21a9</p> </li> </ol>"},{"location":"optimizers/adam/","title":"Adam: Adaptive Moment Estimation","text":"<p>Adam (Adaptive Moment Estimation) computes per-parameter adaptive learning rates from the first and second gradient moments. Adam combines the advantages of two other optimizers: AdaGrad, which adapts the learning rate to the parameters, and RMSProp, which uses a moving average of squared gradients to set per-parameter learning rates. Adam also introduces bias-corrected estimates of the first and second gradient averages.</p> <p>Adam was introduced by Diederik Kingma and Jimmy Ba in Adam: A Method for Stochastic Optimization.</p>"},{"location":"optimizers/adam/#hyperparameters","title":"Hyperparameters","text":"<p>optimi sets the default \\(\\beta\\)s to <code>(0.9, 0.99)</code> and default \\(\\epsilon\\) to <code>1e-6</code>. These values reflect current best-practices and usually outperform the PyTorch defaults.</p> <p>If training on large batch sizes or observing training loss spikes, consider reducing \\(\\beta_2\\) between \\([0.95, 0.99)\\).</p> <p>optimi\u2019s implementation of Adam combines Adam with both AdamW <code>decouple_wd=True</code> and Adam with fully decoupled weight decay <code>decouple_lr=True</code>. Weight decay will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/adam/#optimi.adam.Adam","title":"Adam","text":"<p>Adam optimizer. Optionally with decoupled weight decay (AdamW).</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))</p> <code>(0.9, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_wd</code> and <code>decouple_lr</code> are False, applies L2 penalty (default: 0)</p> <code>0</code> <code>eps</code> <code>float</code> <p>Added to denominator to improve numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>decouple_wd</code> <code>bool</code> <p>Apply decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it can be significantly faster (default: None)</p> <code>None</code> <code>triton</code> <code>bool | None</code> <p>Enables Triton implementation. If unspecified, tries to use Triton as it is significantly faster than both for-loop and foreach implementations (default: None)</p> <code>None</code> <code>gradient_release</code> <code>bool</code> <p>Fuses optimizer step and zero_grad as part of the parameter's backward pass. Requires model hooks created with <code>register_gradient_release</code>. Incompatible with closure (default: False)</p> <code>False</code>"},{"location":"optimizers/adam/#algorithm","title":"Algorithm","text":"<p>Adam with L2 regularization.</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textbf{Adam}\\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\: \\text{(epsilon)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1}) - \\lambda\\bm{\\theta}_{t-1}\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t \\bigl( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) \\bigr)\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>optimi\u2019s Adam also supports AdamW\u2019s decoupled weight decay and fully decoupled weight decay, which are not shown.</p>"},{"location":"optimizers/adamw/","title":"AdamW: Adam with Decoupled Weight Decay","text":"<p>AdamW improves upon Adam by decoupling weight decay from the gradients and instead applying weight decay directly to the model parameters. This modification allows AdamW to achieve better convergence and generalization than Adam.</p> <p>AdamW was introduced by Ilya Loshchilov and Frank Hutter in Decoupled Weight Decay Regularization.</p>"},{"location":"optimizers/adamw/#hyperparameters","title":"Hyperparameters","text":"<p>optimi sets the default \\(\\beta\\)s to <code>(0.9, 0.99)</code> and default \\(\\epsilon\\) to <code>1e-6</code>. These values reflect current best-practices and usually outperform the PyTorch defaults.</p> <p>If training on large batch sizes or observing training loss spikes, consider reducing \\(\\beta_2\\) between \\([0.95, 0.99)\\).</p> <p>optimi\u2019s implementation of AdamW also supports fully decoupled weight decay <code>decouple_lr=True</code>. The default weight decay of 0.01 will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/adamw/#optimi.adamw.AdamW","title":"AdamW","text":"<p>AdamW optimizer: Adam with decoupled weight decay.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))</p> <code>(0.9, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_lr</code> is False, applies decoupled weight decay (default: 1e-2)</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Added to denominator to improve numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of decoupled weight decay (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it can be significantly faster (default: None)</p> <code>None</code> <code>triton</code> <code>bool | None</code> <p>Enables Triton implementation. If unspecified, tries to use Triton as it is significantly faster than both for-loop and foreach implementations (default: None)</p> <code>None</code> <code>gradient_release</code> <code>bool</code> <p>Fuses optimizer step and zero_grad as part of the parameter's backward pass. Requires model hooks created with <code>register_gradient_release</code>. Incompatible with closure (default: False)</p> <code>False</code>"},{"location":"optimizers/adamw/#algorithm","title":"Algorithm","text":"<p>Adam with decoupled weight decay (AdamW).</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textcolor{#009ddb}{\\textbf{Adam}} \\: \\textcolor{#9a3fe4}{\\text{with decoupled weigh decay (AdamW)}} \\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\: \\text{(epsilon)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1}) \\textcolor{#009ddb}{- \\lambda\\bm{\\theta}_{t-1}}\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t \\bigl( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) \\textcolor{#9a3fe4}{+ \\lambda\\bm{\\theta}_{t-1}} \\bigr)\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>optimi\u2019s AdamW also supports fully decoupled weight decay, which is not shown.</p>"},{"location":"optimizers/adan/","title":"Adan: ADAptive Nesterov Momentum","text":"<p>Adan uses a efficient Nesterov momentum estimation method to avoid the extra computation and memory overhead of calculating the extrapolation point gradient. In contrast to other Nesterov momentum estimating optimizers, Adan estimates both the first- and second-order gradient movements. This estimation requires two additional buffers over AdamW, increasing memory usage.</p> <p>Adan was introduced by Xie et al in Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models.</p>"},{"location":"optimizers/adan/#hyperparameters","title":"Hyperparameters","text":"<p>Hyperparameter notes from Xie et al:</p> <ol> <li>\\(\\beta_2\\) is the least sensitive Adan hyperparameter, default of 0.92 works for majority of tasks</li> <li>Xie et al primarily tune \\(\\beta_3\\) (between 0.9-0.999) before \\(\\beta_1\\) (between 0.9-0.98) for different tasks</li> <li>Adan pairs well with large learning rates. Paper and GitHub report up to 3x larger than Lamb and up to 5-10x larger than AdamW</li> <li>Xie et al use the default weight decay of 0.02 for all tasks except fine-tuning BERT (0.01) and reinforcement learning (0)</li> </ol> <p>optimi\u2019s implementation of Adan also supports fully decoupled weight decay <code>decouple_lr=True</code>. The default weight decay of 0.02 will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p> Note: Adan in bfloat16 is Noisier then Other Optimizers <p>Even with Kahan summation, training with Adan in bfloat16 results in noisier updates relative to float32 or mixed precision training than other optimizers.</p>"},{"location":"optimizers/adan/#optimi.adan.Adan","title":"Adan","text":"<p>Adan Optimizer: Adaptive Nesterov Momentum Algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float, float]</code> <p>Coefficients for gradient, gradient difference, and squared gradient moving averages (default: (0.98, 0.92, 0.99))</p> <code>(0.98, 0.92, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_lr</code> is False, applies decoupled weight decay (default: 2e-2)</p> <code>0.02</code> <code>eps</code> <code>float</code> <p>Added to denominator to improve numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of decoupled weight decay (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>adam_wd</code> <code>bool</code> <p>Apply weight decay before parameter update (Adam-style), instead of after the update per Adan algorithm (default: False)</p> <code>False</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>False</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it can be significantly faster (default: None)</p> <code>None</code> <code>triton</code> <code>bool | None</code> <p>Enables Triton implementation. If unspecified, tries to use Triton as it is significantly faster than both for-loop and foreach implementations (default: None)</p> <code>None</code> <code>gradient_release</code> <code>bool</code> <p>Fuses optimizer step and zero_grad as part of the parameter's backward pass. Requires model hooks created with <code>register_gradient_release</code>. Incompatible with closure (default: False)</p> <code>False</code>"},{"location":"optimizers/adan/#algorithm","title":"Algorithm","text":"<p>Adan: Adaptive Nesterov Momentum.</p> \\[ \\begin{align*}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textbf{Adan}  \\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2, \\beta_3 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\: \\text{(epsilon)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}; \\: \\bm{n}_{0} \\leftarrow \\bm{0}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1})\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) (\\bm{g}_t - \\bm{g}_{t-1})\\\\         &amp;\\hspace{10mm} \\bm{n}_t \\leftarrow \\beta_3 \\bm{n}_{t-1} + (1 - \\beta_3)\\bigl(\\bm{g}_t + \\beta_2(\\bm{g}_t - \\bm{g}_{t-1})\\bigr)^2\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{n}}_t \\leftarrow \\bm{n}_t/(1 - \\beta_3^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{\\eta}_t \\leftarrow \\gamma_t/(\\sqrt{\\hat{\\bm{n}}_t} + \\epsilon)\\\\         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow (1+\\gamma_t\\lambda )^{-1}\\bigl(\\bm{\\theta}_{t-1} - \\bm{\\eta}_t (\\hat{\\bm{m}}_t + \\beta_2\\hat{\\bm{v}}_t)\\bigr)\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{align*} \\] <p>During the first step, \\(\\bm{g}_t - \\bm{g}_{t-1}\\) is set to \\(\\bm{0}\\).</p> <p>optimi\u2019s Adan also supports Adam-style weight decay and fully decoupled weight decay, both which are not shown.</p>"},{"location":"optimizers/lion/","title":"Lion: EvoLved Sign Momentum","text":"<p>Lion only keeps track of the gradient moving average (momentum) which can reduce memory usage compared to AdamW. Lion uses two momentum EMA factors, one for tracking momentum and another for using momentum in the update step. Using default hyperparameters, this allows up to ten times longer history for momentum tracking while leveraging more of the current gradient for the model update. Unlike most optimizers, Lion uses the same magnitude for each parameter update calculated using the sign operation.</p> <p>Lion was introduced by Chen et al in Symbolic Discovery of Optimization Algorithms.</p>"},{"location":"optimizers/lion/#hyperparameters","title":"Hyperparameters","text":"<p>Hyperparameter notes from Chen et al:</p> <ol> <li>Due to the larger update norm from the sign operation, a good Lion learning rate is typically 3-10X smaller than AdamW.</li> <li>Since the effective weight decay is multiplied by the learning rate<sup>1</sup>, weight decay should be increased by the learning rate decrease (3-10X).</li> <li>Except for language modeling, \\(\\beta\\)s are set to <code>(0.9, 0.99)</code>. When training T5, Chen at al set \\(\\beta_1=0.95\\) and \\(\\beta_2=0.98\\). Reducing \\(\\beta_2\\) results in better training stability due to less historical memorization.</li> <li>The optimal batch size for Lion is 4096 (vs AdamW\u2019s 256), but Lion still performs well at a batch size of 64 and matches or exceeds AdamW on all tested batch sizes.</li> </ol> <p>optimi\u2019s implementation of Lion also supports fully decoupled weight decay <code>decouple_lr=True</code>. If using fully decoupled weight decay do not increase weight decay. Rather, weight decay will likely need to be reduced as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/lion/#optimi.lion.Lion","title":"Lion","text":"<p>Lion optimizer. Evolved Sign Momentum.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients for update moving average and gradient moving average (default: (0.9, 0.99))</p> <code>(0.9, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_lr</code> is False, applies decoupled weight decay (default: 0)</p> <code>0</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of decoupled weight decay (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it can be significantly faster (default: None)</p> <code>None</code> <code>triton</code> <code>bool | None</code> <p>Enables Triton implementation. If unspecified, tries to use Triton as it is significantly faster than both for-loop and foreach implementations (default: None)</p> <code>None</code> <code>gradient_release</code> <code>bool</code> <p>Fuses optimizer step and zero_grad as part of the parameter's backward pass. Requires model hooks created with <code>register_gradient_release</code>. Incompatible with closure (default: False)</p> <code>False</code>"},{"location":"optimizers/lion/#algorithm","title":"Algorithm","text":"<p>Lion: Evolved Sign Momentum.</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textbf{Lion} \\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1})\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{u} \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_2 \\bm{m}_{t-1} + (1 - \\beta_2) \\bm{g}_t\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t \\bigl(\\text{sign}(\\bm{u}) + \\lambda\\bm{\\theta}_{t-1} \\bigr)\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>optimi\u2019s Lion also supports fully decoupled weight decay, which is not shown.</p> <ol> <li> <p>The learning rate does not modify the effective weight decay when using fully decoupled weight decay.\u00a0\u21a9</p> </li> </ol>"},{"location":"optimizers/radam/","title":"RAdam: Rectified Adam","text":"<p>RAdam (Rectified Adam) is a variant of Adam which improves Adam\u2019s convergence by fixing the adaptive learning rate's large variance during early stages of training. RAdam estimates the variance of the squared gradient moving average and scales the update by this term to rectify the variance. RAdam is comparable to using a learning rate warmup schedule.</p> <p>RAdam was introduced by Liu et al in On the Variance of the Adaptive Learning Rate and Beyond.</p>"},{"location":"optimizers/radam/#hyperparameters","title":"Hyperparameters","text":"<p>optimi sets the default \\(\\beta\\)s to <code>(0.9, 0.99)</code> and default \\(\\epsilon\\) to <code>1e-6</code>. These values reflect current best-practices and usually outperform the PyTorch defaults.</p> <p>If training on large batch sizes or observing training loss spikes, consider reducing \\(\\beta_2\\) between \\([0.95, 0.99)\\).</p> <p>optimi\u2019s implementation of RAdam supports both decoupled weight decay <code>decouple_wd=True</code> and fully decoupled weight decay <code>decouple_lr=True</code>. Weight decay will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/radam/#optimi.radam.RAdam","title":"RAdam","text":"<p>Rectified Adam optimizer. Optionally with decoupled weight decay.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))</p> <code>(0.9, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_wd</code> and <code>decouple_lr</code> are False, applies L2 penalty (default: 0)</p> <code>0</code> <code>eps</code> <code>float</code> <p>Added to denominator to improve numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>decouple_wd</code> <code>bool</code> <p>Apply decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it can be significantly faster (default: None)</p> <code>None</code> <code>triton</code> <code>bool | None</code> <p>Enables Triton implementation. If unspecified, tries to use Triton as it is significantly faster than both for-loop and foreach implementations (default: None)</p> <code>None</code> <code>gradient_release</code> <code>bool</code> <p>Fuses optimizer step and zero_grad as part of the parameter's backward pass. Requires model hooks created with <code>register_gradient_release</code>. Incompatible with closure (default: False)</p> <code>False</code>"},{"location":"optimizers/radam/#algorithm","title":"Algorithm","text":"<p>RAdam: Rectified Adam.</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textcolor{#9a3fe4}{\\textbf{Rectified}} \\: \\textbf{Adam}\\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\: \\text{(epsilon)};\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}; \\: \\textcolor{#9a3fe4}{\\rho_{\\infty} \\leftarrow 2 / (1 - \\beta_2) - 1}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1}) - \\lambda\\bm{\\theta}_{t-1}\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\rho_t \\leftarrow \\rho_{\\infty} - 2 t \\beta^t_2 /(1 - \\beta_2^t)}\\\\[0.5em]         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\textbf{if} \\: \\rho_t &gt; 5\\text{:}}\\\\         &amp;\\hspace{15mm} \\textcolor{#9a3fe4}{r_t \\leftarrow \\sqrt{\\tfrac{(\\rho_t - 4)(\\rho_t - 2)\\rho_{\\infty}}{(\\rho_{\\infty} - 4)(\\rho_{\\infty} -2 ) \\rho_t}}}\\\\         &amp;\\hspace{15mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t \\textcolor{#9a3fe4}{r_t} \\bigl( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) \\bigr)\\\\         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\textbf{else}\\text{:}}\\\\         &amp;\\hspace{15mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t \\textcolor{#9a3fe4}{\\hat{\\bm{m}}_t}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>optimi\u2019s RAdam also supports decoupled weight decay and fully decoupled weight decay, which are not shown.</p>"},{"location":"optimizers/ranger/","title":"Ranger: RAdam and LookAhead","text":"<p>Ranger combines RAdam and Lookahead together in one optimizer. RAdam fixes the adaptive learning rate's large variance during early stages of training to improve convergence and reducing the need for warmup. Lookahead updates model weights like normal, but every k steps interpolates them with a copy of slow moving weights. This moving average of the model weights is less sensitive to suboptimal hyperparameters and reduces the need for hyperparameter tuning.</p> <p>Ranger was introduced by Less Wright in New Deep Learning Optimizer, Ranger: Synergistic combination of RAdam + Lookahead for the best of both.</p>"},{"location":"optimizers/ranger/#hyperparameters","title":"Hyperparameters","text":"<p>Ranger works best with a flat learning rate followed by a short learning rate decay. Try raising the learning rate 2-3x larger than AdamW.</p> <p>optimi sets the default \\(\\beta\\)s to <code>(0.9, 0.99)</code> and default \\(\\epsilon\\) to <code>1e-6</code>. These values reflect current best-practices and usually outperform the PyTorch defaults.</p> <p>optimi\u2019s implementation of Ranger supports both decoupled weight decay <code>decouple_wd=True</code> and fully decoupled weight decay <code>decouple_lr=True</code>. Weight decay will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/ranger/#optimi.ranger.Ranger","title":"Ranger","text":"<p>Ranger optimizer. RAdam with Lookahead.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))</p> <code>(0.9, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_wd</code> and <code>decouple_lr</code> are False, applies L2 penalty (default: 0)</p> <code>0</code> <code>eps</code> <code>float</code> <p>Added to denominator to improve numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>k</code> <code>int</code> <p>Lookahead synchronization period (default: 6)</p> <code>6</code> <code>alpha</code> <code>float</code> <p>Lookahead weight interpolation coefficient (default: 0.5)</p> <code>0.5</code> <code>decouple_wd</code> <code>bool</code> <p>Apply decoupled weight decay instead of L2 penalty (default: True)</p> <code>True</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it can be significantly faster (default: None)</p> <code>None</code> <code>triton</code> <code>bool | None</code> <p>Enables Triton implementation. If unspecified, tries to use Triton as it is significantly faster than both for-loop and foreach implementations (default: None)</p> <code>None</code> <code>gradient_release</code> <code>bool</code> <p>Fuses optimizer step and zero_grad as part of the parameter's backward pass. Requires model hooks created with <code>register_gradient_release</code>. Incompatible with closure (default: False)</p> <code>False</code>"},{"location":"optimizers/ranger/#algorithm","title":"Algorithm","text":"<p>Ranger: RAdam and LookAhead.</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textbf{Ranger: RAdam and} \\: \\textcolor{#9a3fe4}{\\textbf{LookAhead}}\\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\: \\text{(epsilon)};\\\\     &amp;\\hspace{17.25mm} \\bm{\\phi}_0 \\: \\text{(slow params)}; \\: k \\: \\text{(sync)}; \\: \\alpha \\: \\text{(interpolation)};\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\textcolor{#9a3fe4}{\\bm{\\phi}_0 \\leftarrow  \\bm{\\theta}_0}; \\: \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}; \\: \\rho_{\\infty} \\leftarrow 2 / (1 - \\beta_2) - 1;\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1})\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\rho_t \\leftarrow \\rho_{\\infty} - 2 t \\beta^t_2 /(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\textbf{if} \\: \\rho_t &gt; 5\\text{:}\\\\         &amp;\\hspace{15mm} r_t \\leftarrow \\sqrt{\\tfrac{(\\rho_t - 4)(\\rho_t - 2)\\rho_{\\infty}}{(\\rho_{\\infty} - 4)(\\rho_{\\infty} -2 ) \\rho_t}}\\\\         &amp;\\hspace{15mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t r_t \\bigl( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) + \\lambda\\bm{\\theta}_{t-1} \\bigr)\\\\         &amp;\\hspace{10mm} \\textbf{else}\\text{:}\\\\         &amp;\\hspace{15mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t (\\hat{\\bm{m}}_t + \\lambda\\bm{\\theta}_{t-1})\\\\[0.5em]         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\textbf{if} \\: t \\equiv 0 \\pmod{k}\\text{:}}\\\\         &amp;\\hspace{15mm} \\textcolor{#9a3fe4}{\\bm{\\phi}_t \\leftarrow \\bm{\\phi}_{t-k} + \\alpha(\\bm{\\theta}_t - \\bm{\\phi}_{t-k} )}\\\\         &amp;\\hspace{15mm} \\textcolor{#9a3fe4}{\\bm{\\theta}_t  \\leftarrow \\bm{\\phi}_t}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>optimi\u2019s Ranger also supports L2 regularization and fully decoupled weight decay, which are not shown.</p>"},{"location":"optimizers/sgd/","title":"SGD: Stochastic Gradient Descent","text":"<p>Stochastic Gradient Descent iteratively updates the model parameters using the gradient from a mini-batch of data.</p> <p>SGD can be traced back to Herbert Robbins and Sutton Monro\u2019s stochastic approximation methods. Frank Rosenblatt was the first to use SGD to train neural networks in The perceptron: A probabilistic model for information storage and organization in the brain.</p>"},{"location":"optimizers/sgd/#hyperparmeters","title":"Hyperparmeters","text":"<p>SGD supports dampening <code>dampening=True</code>, where <code>dampening=1-momentum</code>. To match PyTorch\u2019s dampening set <code>torch_init=True</code>. This will initialize momentum buffer with first gradient instead of zeroes.</p> <p>optimi\u2019s implementation of SGD also supports decoupled weight decay <code>decouple_wd=True</code> and fully decoupled weight decay <code>decouple_lr=True</code>. Weight decay will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/sgd/#optimi.sgd.SGD","title":"SGD","text":"<p>SGD optimizer. Optionally with momentum and decoupled weight decay.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>momentum</code> <code>float</code> <p>Momentum factor. Gradient moving average coefficient if <code>dampening</code> is True (default: 0)</p> <code>0</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_wd</code> and <code>decouple_lr</code> are False, applies L2 penalty (default: 0)</p> <code>0</code> <code>dampening</code> <code>bool</code> <p>Use dampening for momentum update (default: False)</p> <code>False</code> <code>decouple_wd</code> <code>bool</code> <p>Apply decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>torch_init</code> <code>bool</code> <p>Initialize momentum buffer with first gradient instead of zeroes. Enable to match PyTorch SGD when using dampening (default: False)</p> <code>False</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it can be significantly faster (default: None)</p> <code>None</code> <code>triton</code> <code>bool | None</code> <p>Enables Triton implementation. If unspecified, tries to use Triton as it is significantly faster than both for-loop and foreach implementations (default: None)</p> <code>None</code> <code>gradient_release</code> <code>bool</code> <p>Fuses optimizer step and zero_grad as part of the parameter's backward pass. Requires model hooks created with <code>register_gradient_release</code>. Incompatible with closure (default: False)</p> <code>False</code>"},{"location":"optimizers/sgd/#algorithm","title":"Algorithm","text":"<p>SGD with L2 regularization.</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textcolor{#dc3918}{\\textbf{SGD}} \\: \\textcolor{#009ddb}{\\text{with momentum}} \\: \\textcolor{#9a3fe4}{\\text{and dampening}}\\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta \\: \\text{(momentum)}; \\: \\lambda \\: \\text{(weight decay)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\textcolor{#009ddb}{\\bm{m}_{0} \\leftarrow \\bm{0}}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1}) - \\lambda\\bm{\\theta}_{t-1}\\\\         &amp;\\hspace{10mm} \\textcolor{#009ddb}{\\bm{m}_t \\leftarrow \\beta \\bm{m}_{t-1} +} \\textcolor{#9a3fe4}{(1 - \\beta)} \\textcolor{#009ddb}{\\bm{g}_t}\\\\         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} \\textcolor{#dc3918}{- \\gamma_t\\bm{g}_t} \\textcolor{#009ddb}{- \\gamma_t\\bm{m}_t}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>The SGD update terms \\(\\gamma_t\\bm{g}_t\\) and \\(\\gamma_t\\bm{m}_t\\) are exclusive, applying for SGD and SGD with momentum (and dampening), respectively. The dampening term \\((1 - \\beta)\\) is added to the momentum update \\(\\bm{m}_t \\leftarrow \\beta \\bm{m}_{t-1} + \\bm{g}_t\\) if enabled.</p> <p>optimi\u2019s SGD also supports AdamW\u2019s decoupled weight decay and fully decoupled weight decay, which are not shown.</p>"},{"location":"optimizers/stableadamw/","title":"StableAdamW: AdamW with Update Clipping","text":"<p>StableAdamW is a AdamW-Adafactor hybrid, porting Adafactor\u2019s update clipping into AdamW as a per parameter learning rate modification. StableAdamW\u2019s update clipping outperforms gradient clipping on downstream tasks while avoiding model training instability.</p> <p>StableAdamW was introduced by Wortsman et al in Stable and low-precision training for large-scale vision-language models.</p>"},{"location":"optimizers/stableadamw/#hyperparameters","title":"Hyperparameters","text":"<p>StableAdamW is a drop-in replacement for AdamW and uses the same hyperparameters, with one exception: StableAdamW removes the need for gradient clipping.</p> <p>If training on large batch sizes or still observing training loss spikes, consider reducing \\(\\beta_2\\) between \\([0.95, 0.99)\\).</p> <p>optimi\u2019s implementation of StableAdamW also supports fully decoupled weight decay <code>decouple_lr=True</code>. The default weight decay of 0.01 will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/stableadamw/#optimi.stableadamw.StableAdamW","title":"StableAdamW","text":"<p>StableAdamW optimizer. An AdamW-Adafactor hybrid with learning rate update clipping.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))</p> <code>(0.9, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_lr</code> is False, applies decoupled weight decay (default: 1e-2)</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Added to denominator to improve numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of decoupled weight decay (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it can be significantly faster (default: None)</p> <code>None</code> <code>triton</code> <code>bool | None</code> <p>Enables Triton implementation. If unspecified, tries to use Triton as it is significantly faster than both for-loop and foreach implementations (default: None)</p> <code>None</code> <code>gradient_release</code> <code>bool</code> <p>Fuses optimizer step and zero_grad as part of the parameter's backward pass. Requires model hooks created with <code>register_gradient_release</code>. Incompatible with closure (default: False)</p> <code>False</code>"},{"location":"optimizers/stableadamw/#algorithm","title":"Algorithm","text":"<p>StableAdam with decoupled weight decay (StableAdamW).</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textbf{\\textcolor{#9a3fe4}{Stable}AdamW} \\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\: \\text{(epsilon)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1})\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\textbf{RMS}_t \\leftarrow  \\sqrt{\\mathbb{E[\\bm{g}^2_t/\\text{max}(\\bm{v}_t, \\epsilon^2)]}}}\\\\         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\bm{\\eta}_t \\leftarrow  \\gamma_t/\\text{max}(1,\\textbf{RMS}_t)}\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\textcolor{#9a3fe4}{\\bm{\\eta}_t} \\bigl( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) + \\lambda\\bm{\\theta}_{t-1} \\bigr)\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>Following Stable and low-precision training for large-scale vision-language models, the \\(\\text{RMS}_t\\) steps occur independantly for each tensor. Likewise, the \\(\\text{max}(\\bm{v}_t, \\epsilon^2)\\) term, instead of \\(\\sqrt{\\mathbb{E[\\bm{g}^2_t/\\bm{v}_t]}}\\), is added to prevent division by zero issues.</p> <p>optimi\u2019s StableAdamW also supports fully decoupled weight decay, which is not shown.</p>"}]}

<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Fast, Modern, and Low Precision PyTorch Optimizers">
      
      
      
        <link rel="canonical" href="https://optimi.benjaminwarner.dev/">
      
      
      
      
        
      
      
      <link rel="icon" href="images/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>optimi - optimī</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetbrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Jetbrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="css/extra.css">
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="swiss" data-md-color-primary="swiss" data-md-color-accent="swiss">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#optimi" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      <!-- Determine classes -->


  


<!-- Header -->
<header class="md-header md-header--shadow" data-md-component="header">
  <nav
    class="md-header__inner md-grid"
    aria-label="Header"
  >

    <!-- Link to home -->
    <a
      href="."
      title="optimī"
      aria-label="optimī"
    >
      <span class="custom-title">
        optimī
      </span>
    </a>

    <!-- Button to open drawer -->
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>

    <!-- Header title -->
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis"></span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              optimi
            
          </span>
        </div>
      </div>
    </div>

    <!-- Color palette toggle -->
    
      
    

    <!-- Site language selector -->
    

    <!-- Button to open search modal -->
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>

      <!-- Search interface -->
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    

    <!-- Repository information -->
    
      <div class="md-header__source">
        <a href="https://github.com/warner-benjamin/optimi" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    warner-benjamin/optimi
  </div>
</a>
      </div>
    
  </nav>

  <!-- Navigation tabs (sticky) -->
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<!-- Determine classes -->




<!-- Navigation -->
<nav
  class="md-nav md-nav--primary"
  aria-label="Navigation"
  data-md-level="0"
>

  <!-- Site title -->
  <label class="md-nav__title" for="__drawer">
    <a
      href="."
      title="optimī"
      class="md-nav__button md-logo"
      aria-label="optimī"
      data-md-component="logo"
    >
      optimī
    </a>
    optimī
  </label>

  <!-- Repository information -->
  
    <div class="md-nav__source">
      <a href="https://github.com/warner-benjamin/optimi" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    warner-benjamin/optimi
  </div>
</a>
    </div>
  

  <!-- Navigation list -->
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="kahan_summation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Low Precision Training
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="triton/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Triton Optimizers
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="fully_decoupled_weight_decay/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Fully Decoupled Weight Decay
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="gradient_release/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Gradient Release
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizer_accumulation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Optimizer Accumulation
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="foreach/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ForEach Optimizers
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="which_optimizer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Which Optimizer?
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Optimizers
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Optimizers
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizers/adam/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adam
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizers/adamw/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AdamW
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizers/adan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adan
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizers/lion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lion
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizers/radam/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    RAdam
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizers/ranger/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ranger
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizers/sgd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    SGD
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizers/stableadamw/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    StableAdamW
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="utils/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Utilities
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#low-precision-training-with-kahan-summation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Low Precision Training with Kahan Summation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fast-triton-implementations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fast Triton Implementations
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fully-decoupled-weight-decay" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fully Decoupled Weight Decay
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-release-fused-backward-and-optimizer-step" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gradient Release: Fused Backward and Optimizer Step
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimizer-accumulation-gradient-release-and-accumulation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimizer Accumulation: Gradient Release and Accumulation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimizers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimizers
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#install" class="md-nav__link">
    <span class="md-ellipsis">
      
        Install
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usage" class="md-nav__link">
    <span class="md-ellipsis">
      
        Usage
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#differences-from-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      
        Differences from PyTorch
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="optimi">optimī<a class="headerlink" href="#optimi" title="Permanent link">¶</a></h1>
<p><strong>Fast, Modern, and Low Precision PyTorch Optimizers</strong></p>
<p>optimi enables accurate low precision training via Kahan summation, integrates gradient release and optimizer accumulation for additional memory efficiency, supports fully decoupled weight decay, and features fast implementations of modern optimizers.</p>
<h2 id="low-precision-training-with-kahan-summation">Low Precision Training with Kahan Summation<a class="headerlink" href="#low-precision-training-with-kahan-summation" title="Permanent link">¶</a></h2>
<p>optimi optimizers can match the performance of mixed precision when <a href="kahan_summation/">training in pure BFloat16 by using Kahan summation</a>.</p>
<p><img alt="" src="https://ghp-cdn.benjaminwarner.dev/optimi/kahan_pretrain.png"></p>
<p>Training in pure BFloat16 with Kahan summation can reduce non-activation training memory usage up to <a href="kahan_summation/#memory-savings">37 to 45 percent</a> when using an Adam optimizer. BFloat16 training can increase single GPU <a href="kahan_summation/#training-speedup">training speed up to 10 percent</a> at the same batch size.</p>
<h2 id="fast-triton-implementations">Fast Triton Implementations<a class="headerlink" href="#fast-triton-implementations" title="Permanent link">¶</a></h2>
<p>optimi's fused <a href="triton/">Triton optimizers</a> are faster than PyTorch's fused Cuda optimizers, and nearly as fast as compiled optimizers without any hassle.</p>
<p><img alt="" src="https://ghp-cdn.benjaminwarner.dev/optimi/adamw_speed.png"></p>
<p>optimi's Triton backend supports modern NVIDIA (Ampere or newer), AMD, and Intel GPUs, and is enabled by default for all optimizers.</p>
<h2 id="fully-decoupled-weight-decay">Fully Decoupled Weight Decay<a class="headerlink" href="#fully-decoupled-weight-decay" title="Permanent link">¶</a></h2>
<p>In addition to supporting PyTorch-style decoupled weight decay, optimi optimizers also support <a href="fully_decoupled_weight_decay/">fully decoupled weight decay</a>.</p>
<p>Fully decoupled weight decay decouples weight decay from the learning rate, more accurately following <a href="https://arxiv.org/abs/1711.05101"><em>Decoupled Weight Decay Regularization</em></a>. This can help simplify hyperparameter tuning as the optimal weight decay is no longer tied to the learning rate.</p>
<h2 id="gradient-release-fused-backward-and-optimizer-step">Gradient Release: Fused Backward and Optimizer Step<a class="headerlink" href="#gradient-release-fused-backward-and-optimizer-step" title="Permanent link">¶</a></h2>
<p>optimi optimizers can perform the <a href="gradient_release/">optimization step layer-by-layer during the backward pass</a>, immediately freeing gradient memory.</p>
<p>Unlike the current PyTorch implementation, optimi’s gradient release optimizers are a drop-in replacement for standard optimizers and seamlessly work with exisiting hyperparmeter schedulers.</p>
<h2 id="optimizer-accumulation-gradient-release-and-accumulation">Optimizer Accumulation: Gradient Release and Accumulation<a class="headerlink" href="#optimizer-accumulation-gradient-release-and-accumulation" title="Permanent link">¶</a></h2>
<p>optimi optimizers can approximate gradient accumulation with gradient release by <a href="optimizer_accumulation/">accumulating gradients into the optimizer states</a>.</p>
<h2 id="optimizers">Optimizers<a class="headerlink" href="#optimizers" title="Permanent link">¶</a></h2>
<p>optimi implements the following optimizers:</p>
<ul>
<li><a href="optimizers/adam/">Adam</a></li>
<li><a href="optimizers/adamw/">AdamW</a></li>
<li><a href="optimizers/adan/">Adan</a></li>
<li><a href="optimizers/lion/">Lion</a></li>
<li><a href="optimizers/radam/">RAdam</a></li>
<li><a href="optimizers/ranger/">Ranger</a></li>
<li><a href="optimizers/sgd/">SGD</a></li>
<li><a href="optimizers/stableadamw/">StableAdamW</a></li>
</ul>
<h2 id="install">Install<a class="headerlink" href="#install" title="Permanent link">¶</a></h2>
<p>optimi is available to install from pypi.</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>torch-optimi
</code></pre></div>
<h2 id="usage">Usage<a class="headerlink" href="#usage" title="Permanent link">¶</a></h2>
<p>To use an optimi optimizer with Kahan summation and fully decoupled weight decay:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">optimi</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdamW</span>

<span class="c1"># create or cast model in low precision (bfloat16)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

<span class="c1"># initialize any optimi optimizer with parameters &amp; fully decoupled weight decay</span>
<span class="c1"># Kahan summation is automatically enabled since model &amp; inputs are bfloat16</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">decouple_lr</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># forward and backward, casting input to bfloat16 if needed</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">))</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># optimizer step</span>
<span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>
<p>To use with PyTorch-style weight decay with float32 or mixed precision:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># create model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># initialize any optimi optimizer with parameters</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
</code></pre></div>
<p>To use with gradient release:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># initialize any optimi optimizer with `gradient_release=True`</span>
<span class="c1"># and call `prepare_for_gradient_release` on model and optimizer</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">gradient_release</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">prepare_for_gradient_release</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

<span class="c1"># setup a learning rate scheduler like normal</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>

<span class="c1"># calling backward on the model will peform the optimzier step</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">))</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># optimizer step and zero_grad are no longer needed, and will</span>
<span class="c1"># harmlessly no-op if called by an existing training framework</span>
<span class="c1"># opt.step()</span>
<span class="c1"># opt.zero_grad()</span>

<span class="c1"># step the learning rate scheduler like normal</span>
<span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># optionally remove gradient release hooks when done training</span>
<span class="n">remove_gradient_release</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div>
<p>To use with optimizer accumulation:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># initialize any optimi optimizer with `gradient_release=True`</span>
<span class="c1"># and call `prepare_for_gradient_release` on model and optimizer</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">gradient_release</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">prepare_for_gradient_release</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

<span class="c1"># update model parameters every four steps after accumulating</span>
<span class="c1"># gradients directly into the optimizer states</span>
<span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># setup a learning rate scheduler for gradient accumulation</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>

<span class="c1"># use existing PyTorch dataloader</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="c1"># `optimizer_accumulation=True` accumulates gradients into</span>
    <span class="c1"># optimizer states. set `optimizer_accumulation=False` to</span>
    <span class="c1"># update parameters by performing a full gradient release step</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">optimizer_accumulation</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">!=</span> <span class="mi">0</span>

    <span class="c1"># calling backward on the model will peform the optimizer step</span>
    <span class="c1"># either accumulating gradients or updating model parameters</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># optimizer step and zero_grad are no longer needed, and will</span>
    <span class="c1"># harmlessly no-op if called by an existing training framework</span>
    <span class="c1"># opt.step()</span>
    <span class="c1"># opt.zero_grad()</span>

    <span class="c1"># step the learning rate scheduler after accumulating gradients</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">opt</span><span class="o">.</span><span class="n">optimizer_accumulation</span><span class="p">:</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># optionally remove gradient release hooks when done training</span>
<span class="n">remove_gradient_release</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div>
<h2 id="differences-from-pytorch">Differences from PyTorch<a class="headerlink" href="#differences-from-pytorch" title="Permanent link">¶</a></h2>
<p>optimi optimizers do not support compilation, differentiation, complex numbers, or have capturable versions.</p>
<p>optimi Adam optimizers do not support AMSGrad and SGD does not support Nesterov momentum. Optimizers which debias updates (Adam optimizers and Adan) calculate the debias term per parameter group, not per parameter.</p>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 Benjamin Warner, MIT License
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/warner-benjamin" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/benjamin_warner" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M459.4 151.7c.3 4.5.3 9.1.3 13.6 0 138.7-105.6 298.6-298.6 298.6-59.5 0-114.7-17.2-161.1-47.1 8.4 1 16.6 1.3 25.3 1.3 49.1 0 94.2-16.6 130.3-44.8-46.1-1-84.8-31.2-98.1-72.8 6.5 1 13 1.6 19.8 1.6 9.4 0 18.8-1.3 27.6-3.6-48.1-9.7-84.1-52-84.1-103v-1.3c14 7.8 30.2 12.7 47.4 13.3-28.3-18.8-46.8-51-46.8-87.4 0-19.5 5.2-37.4 14.3-53C87.4 130.8 165 172.4 252.1 176.9c-1.6-7.8-2.6-15.9-2.6-24C249.5 95.1 296.3 48 354.4 48c30.2 0 57.5 12.7 76.7 33.1 23.7-4.5 46.5-13.3 66.6-25.3-7.8 24.4-24.4 44.8-46.1 57.8 21.1-2.3 41.6-8.1 60.4-16.2-14.3 20.8-32.2 39.3-52.6 54.3"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://threads.net/@_benjaminwarner" target="_blank" rel="noopener" title="threads.net" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M331.5 235.7c2.2.9 4.2 1.9 6.3 2.8 29.2 14.1 50.6 35.2 61.8 61.4 15.7 36.5 17.2 95.8-30.3 143.2-36.2 36.2-80.3 52.5-142.6 53h-.3c-70.2-.5-124.1-24.1-160.4-70.2-32.3-41-48.9-98.1-49.5-169.6v-.5c.5-71.5 17.1-128.6 49.4-169.6 36.3-46.1 90.3-69.7 160.5-70.2h.3c70.3.5 124.9 24 162.3 69.9 18.4 22.7 32 50 40.6 81.7l-40.4 10.8c-7.1-25.8-17.8-47.8-32.2-65.4-29.2-35.8-73-54.2-130.5-54.6-57 .5-100.1 18.8-128.2 54.4C72.1 146.1 58.5 194.3 58 256c.5 61.7 14.1 109.9 40.3 143.3 28 35.6 71.2 53.9 128.2 54.4 51.4-.4 85.4-12.6 113.7-40.9 32.3-32.2 31.7-71.8 21.4-95.9-6.1-14.2-17.1-26-31.9-34.9-3.7 26.9-11.8 48.3-24.7 64.8-17.1 21.8-41.4 33.6-72.7 35.3-23.6 1.3-46.3-4.4-63.9-16-20.8-13.8-33-34.8-34.3-59.3-2.5-48.3 35.7-83 95.2-86.4 21.1-1.2 40.9-.3 59.2 2.8-2.4-14.8-7.3-26.6-14.6-35.2-10-11.7-25.6-17.7-46.2-17.8h-.7c-16.6 0-39 4.6-53.3 26.3l-34.4-23.6c19.2-29.1 50.3-45.1 87.8-45.1h.8c62.6.4 99.9 39.5 103.7 107.7l-.2.2zm-156 68.8c1.3 25.1 28.4 36.8 54.6 35.3 25.6-1.4 54.6-11.4 59.5-73.2-13.2-2.9-27.8-4.4-43.4-4.4-4.8 0-9.6.1-14.4.4-42.9 2.4-57.2 23.2-56.2 41.8z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": ".", "features": ["content.code.copy", "navigation.expand", "search.highlight", "search.share"], "search": "assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="js/katex.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>